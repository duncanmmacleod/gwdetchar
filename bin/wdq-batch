#!/usr/bin/env python
# coding=utf-8
# Copyright (C) LIGO Scientific Collaboration (2015-)
#
# This file is part of the GW DetChar python package.
#
# GW DetChar is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# GW DetChar is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with GW DetChar.  If not, see <http://www.gnu.org/licenses/>.

"""Batch-generate a series of Omega-pipeline scans.

GPS times can be given individually on the command-line, one after the other,
or can be bundled into one file formatted where the first column contains
the GPS times (other columns are ignored).

The output of this script is a condor workflow in the form of a DAG file,
with associated condor submit (`.sub`) file and equivalent shell script (`.sh`)
in the output directory.
Submitting the workflow to Condor will result in the scans being processed
in parallel, or you can just run the `.sh` script to process in serial.
"""

from __future__ import print_function

import os
import subprocess
from getpass import getuser

from pycondor import (Dagman, Job)

from gwdetchar import (omega, cli, condor)

# attempt to get WDQ path
WDQ = os.path.join(os.path.dirname(__file__), 'wdq')
if not os.path.isfile(WDQ):
    WDQ = None

# set default accounting information
CONDOR_ACCOUNTING_GROUP = os.getenv(
    '_CONDOR_ACCOUNTING_GROUP', 'ligo.dev.{epoch}.detchar.user_req.omegascan')
CONDOR_ACCOUNTING_USER = os.getenv(
    '_CONDOR_ACCOUNTING_USER', getuser())


# -- parse command line -------------------------------------------------------

parser = cli.create_parser(description=__doc__)

parser.add_argument('gps-time', nargs='+',
                    help='GPS time(s) to scan, or path to a file '
                         'containing a single column of such times')
cli.add_ifo_option(parser)
parser.add_argument('-o', '--output-dir', default=os.getcwd(),
                    help='output directory for all scans')

parser.add_argument(
    '-f', '--config-file',
    help='path to configuration file to use, default: '
         'choose based on observatory, epoch, and pipeline')
parser.add_argument('-q', '--wdq', default=WDQ, required=WDQ is None,
                    help='path to wdq executable')
parser.add_argument('-w', '--wpipeline', default='gwdetchar-omega',
                    nargs='?', const=omega.WPIPELINE,
                    help='path to wpipeline binary (default: %(default)s), '
                         'can choose the Matlab version by passing this flag '
                         'with no argument')
parser.add_argument('--colormap', default=None,
                    help='name of colormap to use (supported either in Python '
                         'or in Matlab omega > r3449), default: choose based '
                         'on wpipeline')

margs = parser.add_argument_group('Matlab options')
margs.add_argument(
    '-c', '--cache-file',
    help='path to data cache file, if not given, data locations '
         'are found using the datafind server, must be in LAL cache format')

pargs = parser.add_argument_group('Python options')
pargs.add_argument('-d', '--disable-correlation', action='store_true',
                   default=False, help='disable cross-correlation of aux '
                                       'channels, default: False')
pargs.add_argument('-t', '--far-threshold', type=float, default=1e-10,
                   help='white noise false alarm rate threshold for '
                        'processing channels, default: %(default)s Hz')
pargs.add_argument('-s', '--ignore-state-flags', action='store_true',
                   default=False, help='ignore state flag definitions in '
                                       'the configuration, default: False')
cli.add_nproc_option(pargs)
pargs.add_argument('-v', '--verbose', action='store_true', default=False,
                   help='print verbose output, default: False')

cargs = parser.add_argument_group('Condor options')
cargs.add_argument('-u', '--universe', default='vanilla', type=str,
                   help='universe for condor processing')
cargs.add_argument('--submit', action='store_true', default=False,
                   help='submit DAG directly to condor queue')
cargs.add_argument('--monitor', action='store_true', default=False,
                   help='monitor the DAG progress after submission; '
                        'only used with --submit')
cargs.add_argument('--condor-accounting-group',
                   default=CONDOR_ACCOUNTING_GROUP,
                   help='accounting_group for condor submission on the LIGO '
                        'Data Grid, include \'{epoch}\' (with curly brackets) '
                        'to auto-substitute the appropriate epoch based on '
                        'the GPS times')
cargs.add_argument('--condor-accounting-group-user',
                   default=CONDOR_ACCOUNTING_USER,
                   help='accounting_group_user for condor submission on the '
                        'LIGO Data Grid')
cargs.add_argument('--condor-timeout', type=float, default=None, metavar='T',
                   help='configure condor to terminate jobs after T hours '
                        'to prevent idling, default: %(default)s')
cargs.add_argument('--condor-command', action='append', default=[],
                   help="Extra condor submit commands to add to "
                        "gw_summary submit file. Can be given "
                        "multiple times in the form \"key=value\"")

args = parser.parse_args()

outdir = os.path.abspath(os.path.expanduser(args.output_dir))

# parse times
times = getattr(args, 'gps-time')

if len(times) == 1:
    try:  # try converting to GPS
        times = [float(times[0])]
    except (TypeError, ValueError):  # otherwise read as file
        import numpy
        times = numpy.loadtxt(times[0], dtype=float, ndmin=1)
else:
    times = list(map(float, times))

# finalise accounting tag based on run
if '{epoch}' in args.condor_accounting_group:
    gpsepoch = max(times)
    epoch = condor.accounting_epoch(gpsepoch)
    args.condor_accounting_group = args.condor_accounting_group.format(
        epoch=epoch.lower())

# valid the accounting tag up-front
try:
    valid = condor.validate_accounting_tag(args.condor_accounting_group)
except EnvironmentError:
    valid = True  # failed to load condor tags, not important
if not valid:
    listtags = 'cat {0} | json_pp | less'.format(condor.ACCOUNTING_GROUPS_FILE)
    raise ValueError("condor accounting tag {0!r} recognised, to see the list "
                     "of valid groups, run `{1}`".format(
                         args.condor_accounting_group, listtags))

# set colormap default
if not args.colormap and args.wpipeline.endswith('wpipeline'):
    args.colormap = 'parula'  # matlab
elif not args.colormap:
    args.colormap = 'viridis'  # python

# -- generate workflow --------------------------------------------------------

# generate directories
logdir = os.path.join(outdir, 'logs')
subdir = os.path.join(outdir, 'condor')

# add custom condor commands, using defaults
condorcmds = [
    "accounting_group = {}".format(args.condor_accounting_group),
    "accounting_group_user = {}".format(args.condor_accounting_group_user),
]
if args.condor_timeout:
    condorcmds.append("periodic_remove = {}".format(
        'CurrentTime-EnteredCurrentStatus > %d' % (3600 * args.condor_timeout),
    ))
condorcmds.extend(args.condor_command)

# generate dagman
tag = 'wdq-batch'
dagman = Dagman(
    name=tag,
    submit=subdir,
)

# create condor job
job = Job(
    dag=dagman,
    name=os.path.basename(args.wdq),
    executable=args.wdq,
    universe=args.universe,
    submit=subdir,
    error=logdir,
    output=logdir,
    getenv=True,
    request_memory=4096 if args.universe != "local" else None,
    extra_lines=condorcmds,
)

# add common wdq options
arguments = [
    "--wpipeline", args.wpipeline,
    "--colormap", args.colormap,
    "--ifo", args.ifo,
]
if args.config_file is not None:
    arguments.extend(("--config-file", os.path.abspath(args.config_file)))
if args.wpipeline.endswith('wpipeline'):
    arguments.append("--condor")
    if args.cache_file is not None:
        arguments.extent(("--cache-file", args.cache_file))
else:
    arguments.extend(("--far-threshold", str(args.far_threshold)))
    arguments.extend(("--nproc", str(args.nproc)))
    if args.disable_correlation:
        arguments.append("--disable-correlation")
    if args.ignore_state_flags:
        arguments.append("--ignore-state-flags")
    if args.verbose:
        arguments.append("--verbose")

# make node in workflow for each time
for t in times:
    cmd = " ".join([str(t)] + arguments)
    job.add_arg(cmd, name=str(t).replace(".", "_"))

# write DAG
dagman.build(fancyname=False)
print("Workflow generated for {} times".format(len(times)))
if args.submit:
    dagman.submit_dag(submit_options="-force")
else:
    print(
        "Submit to condor via:\n\n"
        "$ condor_submit_dag {0.submit_file}".format(dagman),
    )

if args.submit and args.monitor:
    print("Monitoring progress of {0.submit_file}".format(dagman))
    try:
        subprocess.check_call(
            ["pycondor", "monitor", dagman.submit_file],
        )
    except KeyboardInterrupt:
        pass
